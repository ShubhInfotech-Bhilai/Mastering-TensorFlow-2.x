{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow 2.0 Intermediate",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "p0JofRx2if_h",
        "colab_type": "code",
        "outputId": "01648fbc-d38d-463d-c2aa-99942d719a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install tensorflow-gpu==2.0.0.alpha0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0.alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 332.1MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.0.9)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0.alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.16.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0.alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.33.1)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0.alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0.alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0.alpha0) (40.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (0.15.2)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, google-pasta, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n",
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BGLE7fm1l5pG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c033238d-3ca5-417b-ea68-35af421e33ac"
      },
      "cell_type": "code",
      "source": [
        "weight1 = tf.Variable(2.0)\n",
        "weight2 = tf.Variable(3.0)\n",
        "weight3 = tf.Variable(4.0)\n",
        "\n",
        "def weight_sum(x1, x2, x3):\n",
        "    return weight1*x1 + weight2*x2 + weight3*x3\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    sum_val = weight_sum(5., 6., 7.)\n",
        "    # Compute the gradients within the contex only if higher order derivatives are to be calculated otherwise, it's a wasteful process\n",
        "    # So, we compute it outside the context and prevent it from writing on the tape in a persistent tape mode\n",
        "    \n",
        "[weight_grad1] = tape.gradient(sum_val, [weight1])\n",
        "[weight_grad2] = tape.gradient(sum_val, [weight2])\n",
        "[weight_grad3] = tape.gradient(sum_val, [weight3])\n",
        "\n",
        "print(f'The gradients are {weight_grad1}, {weight_grad2} and {weight_grad3}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradients are 5.0, 6.0 and 7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQQkVrGOhgkD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TFRecords\n",
        "Because a TFRecord file is a sequence of binary strings, its structure must be specified prior to saving so that it can be properly written and subsequently read back. TensorFlow has two structures for this, tf.train.Example and tf.train.SequenceExample. What you have to do is store each sample of your data in one of these structures, then serialize it, and use tf.python_io.TFRecordWriter to save it to disk.\n",
        "\n",
        "Here, the float array, data, is converted to the binary format and then saved to disc. \n",
        "A feature is a dictionary containing the data that is passed to tf.train.Example prior to serialization and saving."
      ]
    },
    {
      "metadata": {
        "id": "K3fr0ICFhcN2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "data=np.array([10.,11.,12.,13.,14.,15.])\n",
        "\n",
        "def npy_to_tfrecords(fname,data):\n",
        "    writer = tf.io.TFRecordWriter(fname)\n",
        "    feature={}\n",
        "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    serialized = example.SerializeToString()\n",
        "    writer.write(serialized)\n",
        "    writer.close()\n",
        "\n",
        "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoZ2zIaChsCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code to read the record back is as follows. A parse_function function is constructed that decodes the dataset read back from the file. This requires a dictionary (keys_to_features) with the same name and structure as the saved data:"
      ]
    },
    {
      "metadata": {
        "id": "TBTQj0vYhs6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
        "\n",
        "def parse_function(example_proto):\n",
        " keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
        "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
        "    return parsed_features['data']\n",
        "\n",
        "dataset = dataset.map(parse_function)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "# array is retrieved as one item\n",
        "item = iterator.get_next()\n",
        "print(item)\n",
        "print(item.numpy())\n",
        "print(item[2].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0EKLDynhwho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A more complex use-case:"
      ]
    },
    {
      "metadata": {
        "id": "aDtiBhjah0tb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = './students.tfrecords'\n",
        "data = {\n",
        "            'ID': 61553,\n",
        "            'Name': ['Jones', 'Felicity'],\n",
        "            'Scores': [45.6, 97.2] \n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OuFT01Iih57f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using this, we can construct a tf.train.Example class, again using the Feature() method. Note how we have to encode our string:\n",
        "\n",
        "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[data['ID']]))\n",
        "\n",
        "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in data['Name']]))\n",
        "\n",
        "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=data['Scores']))\n",
        "\n",
        "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSYSatgmiIRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Serializing and writing this record to disc is the same as TFRecord example 1:\n",
        "\n",
        "writer = tf.io.TFRecordWriter(filename)\n",
        "writer.write(example.SerializeToString())\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nj1We2k_iMYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To read this back, we just need to construct our parse_function function to reflect the structure of the record:\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
        "\n",
        "def parse_function(example_proto):\n",
        "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
        "                       'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
        "                        'Scores':tf.io.VarLenFeature(dtype = tf.float32)\n",
        "                       }\n",
        "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
        "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ph2qFbuFiQCa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(parse_function)\n",
        "\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "item = iterator.get_next()\n",
        "# record is retrieved as one item\n",
        "print(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhYOoaTvia3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can extract our data from item (note that the string must be decoded (from bytes) where the default for our Python 3 is utf8). Note also that the string and the array of floats are returned as sparse arrays, and to extract them from the record, we use the sparse array value method:"
      ]
    },
    {
      "metadata": {
        "id": "GHIDftufibUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"ID: \",item[0].numpy())\n",
        "name = item[1].values.numpy()\n",
        "name1= name[0].decode()returned\n",
        "name2 = name[1].decode('utf8')\n",
        "print(\"Name:\",name1,\",\",name2)\n",
        "print(\"Scores: \",item[2].values.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}